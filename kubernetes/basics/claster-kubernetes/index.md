---
metaTitle: Как устроен кластер Kubernetes
metaDescription: Узнайте как устроен кластер Kubernetes - основные компоненты, архитектура, работа с pod, node, контроллерами, сетью и секретами
author: Олег Марков
title: Как устроен кластер Kubernetes
preview: Разберитесь в устройстве кластера Kubernetes - от структуры master-worker до стихий контроля, управления, работы сети и секретов. Примеры, схемы и объяснения помогут вам понять как всё работает вместе
---

## Введение

Архитектура Kubernetes может быть сложной на первый взгляд, но при внимательном рассмотрении она оказывается логичной и удобной для управления масштабируемыми приложениями. Кластер Kubernetes — это совокупность серверов, совместно координирующих запуск контейнеров и сервисов с помощью автоматических механизмов масштабирования, восстановления и обновления. В этой статье вы узнаете, из чего состоит кластер Kubernetes, как он устроен изнутри, какие компоненты отвечают за работу под капотом, а также увидите на практике, как взаимодействуют основные элементы кластера.

---

## Архитектура кластера Kubernetes

### Общая схема

Кластер Kubernetes строится по принципу разделения на управляющую (control plane) и рабочие (worker) ноды:

- **Управляющая нода** (Control Plane, иногда называют master) — координирует работу кластера, принимает решения и распределяет задачи.
- **Рабочие ноды** (Worker nodes) — на них непосредственно запускаются ваши приложения (Pods).

Эта архитектура позволяет разделить ответственность: управляющие ноды отвечают за "мозги" кластера, а рабочие — за работу приложений.

Вот примерная схема, чтобы было понятнее:

```
+---------------------+       +----------------------+
|    Control Plane    |       |     Worker nodes     |
| +-----------------+ |       | +------------------+ |
| | kube-apiserver  | | <---> | | kubelet, Pod     | |
| +-----------------+ |       | +------------------+ |
| | etcd            | |       | +------------------+ |
| +-----------------+ |       | | kube-proxy       | |
| | kube-scheduler  | |       | +------------------+ |
| +-----------------+ |       | | container runtime| |
| | kube-controller | |       | +------------------+ |
| +-----------------+ |       +----------------------+
+---------------------+
```

Давайте подробнее разберём каждый из этих блоков.

---

### Основные компоненты управляющей ноды (Control Plane)

#### kube-apiserver

Это центральная точка входа для всех команд и запросов ко всем компонентам. Все внешние и внутренние взаимодействия проходят через этот компонент.

- Принимает REST-запросы (например, команды `kubectl`).
- Валидирует их и модифицирует объекты состояния кластера.

#### etcd

etcd — распределённое key-value хранение, где Kubernetes хранит всё состояние кластера. Это система, на которую опирается сохранность и отказоустойчивость данных о состоянии кластера.

- Хранит описание всех объектов: pods, сервисы, настройки и т.д.
- Обеспечивает консистентность данных.

#### kube-scheduler

Этот компонент назначает новые pod на worker-ноды, основываясь на ресурсах и других правилах.

- Анализирует свободные ресурсы.
- Решает, куда отправить запрошенный pod.

#### kube-controller-manager

Здесь управляются все контроллеры — процессы, следящие за состоянием кластера и поддерживающие его в нужном виде.

- Следит за replica set, deployment, namespace, endpoints и др.
- Например, если вы задаете 3 реплики приложения, контроллер следит, чтобы всегда были запущены именно эти 3 pod.

#### cloud-controller-manager (опционально)

Связывает кластер с API облачных провайдеров (например, автоматическое создание балансировщика в Amazon или Google Cloud).

---

### Компоненты рабочей ноды (Worker node)

#### kubelet

Здесь происходит выполнение команд, полученных от управляющей ноды:

- kubelet следит за состоянием pod, запускает и завершает их по указанию.
- Отчитывается в управляющую часть о состоянии node.

#### kube-proxy

Отвечает за сетевые прокси и правила маршрутизации на node:

- Организует сетевое взаимодействие между Pod на разных нодах.
- Управляет iptables или ipvs для передачи трафика.

#### Container Runtime

Работа контейнеров (например, Docker, containerd, cri-o) непосредственно обеспечивается этим компонентом:

- Загружает и запускает контейнеры.
- передает сигналы stop/start/kill по команде kubelet.

---

### Как взаимодействуют компоненты на практике

Давайте рассмотрим простой пример: вы хотите развернуть новый deployment через kubectl.

1. Отправляете команду:

    ```bash
    kubectl apply -f nginx-deployment.yaml
    ```

2. `kubectl` шлет запрос в `kube-apiserver`.

3. API server пишет описание нового объекта (Deployment) в `etcd`.

4. Контроллер-сервер отслеживает появление нового Deployment и создает нужное количество ReplicaSet.

5. ReplicaSet создает нужное число Pod (например, 3).

6. Scheduler выбирает подходящие ноды для Pod.

7. kubelet на этих рабочих нодах получает команду запустить контейнеры.

8. Контейнеры стартуют, kube-proxy настраивает сеть между pod.

Как видите, каждый компонент выполняет свою роль, а взаимодействие между ними четко разделено.

---

## Фундаментальные объекты Kubernetes для пользователя

### Pods

Pod — минимальная единица размещения. Обычно в нем один контейнер, реже несколько (например, sidecar-паттерны).

Пример описания Pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hello-world
spec:
  containers:
  - name: hello
    image: busybox
    command: ["echo", "Hello, Kubernetes!"]
```

Комментарии:
- Здесь мы объявляем pod с контейнером, который просто печатает строку в лог.

### ReplicaSet и Deployment

ReplicaSet — объект для контроля количества запущенных pod. Обычно пользователь работает через Deployment — это объект более высокого уровня, управляющий ReplicaSet.

Пример:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
```

Комментарии:
- Вы просите Kubernetes запустить 3 экземпляра nginx.
- Deployment гарантирует, что они всегда будут работать, и может делать обновления без простоя.

### Services

Сервис (Service) предоставляют стабильный способ доступа к группе pod, даже если их физическое размещение меняется.

Пример:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
```

Комментарии:
- ClusterIP позволяет другим pod обращаться к сервису по DNS-имени nginx-service.

### ConfigMap и Secret

Используются для передачи конфигураций и секретных данных контейнерам.

**ConfigMap** пример:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  ENV_TYPE: "production"
  LOG_LEVEL: "info"
```

**Secret** пример:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-password
type: Opaque
data:
  password: bXlfc2VjcmV0X3Bhc3M= # base64(my_secret_pass)
```

Комментарии:
- Секреты передаются в pod в зашифрованном виде (base64).

---

## Сетевые функции в кластере

### Сетевая модель Kubernetes

Kubernetes требует, чтобы все pod в кластере могли напрямую общаться между собой по IP, а также быть доступными извне (через сервисы).

- Подключение обеспечивается с помощью CNI-плагинов (например, Flannel, Calico, Cilium).
- Физически сеть реализуется через наложенные сети (overlay network), туннелирование, виртуальные маршруты и т.д.

### kube-proxy и особенности маршрутизации

kube-proxy следит за изменениями в сервисах и обновляет локальные правила iptables или ipvs, чтобы трафик направлялся к нужным pod.

Пример: если сервис указывает на 3 пода, kube-proxy равномерно распределит запросы между ними.

---

## Мониторинг и масштабирование в Kubernetes

### Monitoring

Для мониторинга обычно используют Prometheus и Grafana. Основные моменты:

- kubelet предоставляет метрики состояния node и pod.
- Есть специальный компонент metrics-server для сбора данных о ресурсе (CPU, память).
- Prometheus собирает метрики по HTTP.

### Автоматическое масштабирование

Для этого используются:

- **Horizontal Pod Autoscaler (HPA):** масштабирует число pod в зависимости от загрузки.
- **Cluster Autoscaler:** автоматически добавляет/убирает worker node по нагрузке.
- **Vertical Pod Autoscaler (VPA):** динамически меняет лимиты ресурсов у pod.

Пример HPA:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deploy
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

Комментарии:
- HPA будет автоматически увеличивать число pod, если средняя загрузка CPU выше 50%.

---

## Как устроена безопасность в Kubernetes кластере

### Аутентификация и авторизация

- kube-apiserver проверяет пользователей по токену, сертификату, OAuth, OpenID.
- После аутентификации действует механизм RBAC (Role-Based Access Control).

### Network Policies

Позволяют ограничивать, кто может обращаться к pod на уровне сети.

Пример:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-nginx
spec:
  podSelector:
    matchLabels:
      app: nginx
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
```
Комментарии:
- Только pod с лейблом frontend могут подключаться к pod nginx.

---

## Практический разбор: развёртывание тестового кластера на Minikube

Для самостоятельного тестирования чаще всего используют Minikube или kind (Kubernetes in Docker).

Запуск Minikube:
```bash
minikube start
```

Проверка компонентов:
```bash
kubectl get nodes
kubectl get pods -A
```

Развёртывание вашего приложения:

```bash
kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
kubectl expose deployment hello-minikube --type=NodePort --port=8080
minikube service hello-minikube
```
Комментарии:
- Вы создаёте деплоймент и сервис. Minikube открывает браузер с работающим приложением.

---

## Как устроен бизнес-контроль в кластере: контроллеры и reconciliation loop

Каждый контроллер в Kubernetes постоянно сравнивает желаемое состояние (указанное в yaml) и текущее состояние кластера. Такой цикл называется reconciliation loop.

- Если значение желаемых реплик не совпадает с текущим — контроллер запускает или удаляет pod.
- Если нужны обновления — контроллер поэтапно обновит контейнеры.

То есть вся работа строится на "самоисправлении": вы описываете что хотите, а кластер добивается этого состояния сам.

---

## Заключение

Кластер Kubernetes — это гибкая, хорошо спроектированная система, в которой чётко выделены функции управления и исполнения, а взаимодействие компонентов автоматизировано и расширяемо. Каждый объект кластера строится вокруг идеи описания желаемого состояния, а реализация этого состояния обеспечивается набором контроллеров и умной организацией сетевого взаимодействия и автоматизации. Основываясь на выстроенной архитектуре, вы можете управлять большим числом приложений, автоматизировать их обновление и масштабирование без ручного вмешательства.

---

## Частозадаваемые технические вопросы по теме статьи и ответы на них

### Как подключиться к удалённому кластеру Kubernetes через kubectl?

Вам нужно получить файл kubeconfig (который обычно выдают администраторы или облачные сервисы), а затем указать kubectl путь к этому файлу:

```bash
export KUBECONFIG=~/path/to/your/kubeconfig
kubectl get nodes
```
Это обеспечит выполнение команд именно в нужном вам кластере.

### Как настроить автоматическое обновление pod при изменении configmap?

ConfigMap по умолчанию не автоматически обновляет конфиг внутри уже бегающих подов. Лучший способ — добавить аннотацию hash или дату модификации в шаблон pod через valueFrom или использовать механизм rollout:

- Меняйте configmap.
- Выполните команду:
  ```bash
  kubectl rollout restart deployment <deployment name>
  ```
Это принудительно перезапустит все pod deployment и подтянет новый configmap.

### Как управлять доступом к API-серверу Kubernetes для разработчиков?

Используйте RBAC (Role-Based Access Control). Создайте нужные роли и назначения (rolebinding) для разных пользователей:

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: read-pods
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```
Затем назначьте эту роль пользователю или сервис-аккаунту через RoleBinding.

### Как устранить ошибку CrashLoopBackOff у pod?

1. Посмотрите логи контейнера:
   ```
   kubectl logs <pod name>
   ```
2. Проверьте конфигурацию ресурса (лимиты CPU, память).
3. Используйте описания pod для поиска причины:
   ```
   kubectl describe pod <pod name>
   ```
4. Обычно ошибка возникает из-за неправильных env переменных, отсутствия конфигов/secret, ошибки в приложении или нехватки ресурсов.

### Как организовать резервное копирование etcd?

Самый прямой вариант — использовать etcdctl:

```bash
etcdctl snapshot save backup.db --endpoints=<endpoint> --cacert=<ca> --cert=<client-cert> --key=<client-key>
```
Регулярно сохраняйте snapshot и храните его в безопасном месте. Это позволит восстановить кластер при потере данных.